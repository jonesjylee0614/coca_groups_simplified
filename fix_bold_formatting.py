#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ reading_materials ç›®å½•ä¸‹æ‰€æœ‰ group*_reading.md æ–‡ä»¶çš„åŠ ç²—æ ¼å¼
- é˜…è¯»ææ–™éƒ¨åˆ†ï¼šåªä¿ç•™è¯æ±‡è¡¨çš„å•è¯åŠ ç²—
- å…¶ä»–éƒ¨åˆ†ï¼šå–æ¶ˆæ‰€æœ‰åŠ ç²—
"""

import re
import os
import sys
from pathlib import Path

# è®¾ç½®è¾“å‡ºç¼–ç 
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def extract_vocabulary_words(content):
    """ä»è¯æ±‡æ³¨é‡Šéƒ¨åˆ†æå–åº”è¯¥åŠ ç²—çš„å•è¯åˆ—è¡¨"""
    vocab_words = set()
    
    # æŸ¥æ‰¾è¯æ±‡æ³¨é‡Šéƒ¨åˆ†ï¼ˆæ”¯æŒå¤šç§æ ‡é¢˜æ ¼å¼ï¼‰
    vocab_section_match = re.search(
        r'## (?:ğŸ“ é‡ç‚¹è¯æ±‡æ³¨é‡Š|Key Vocabulary Annotations).*?(?=## ğŸ¯|## ğŸ§ |## ğŸ“š|## [A-Z]|$)',
        content,
        re.DOTALL | re.IGNORECASE
    )
    
    if vocab_section_match:
        vocab_section = vocab_section_match.group(0)
        
        # æå–æ‰€æœ‰åŠ ç²—çš„å•è¯ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        # æ ¼å¼1: 1. **word** /pronunciation/
        pattern1 = r'\d+\.\s*\*\*([a-zA-Z]+)\*\*\s*/'
        matches1 = re.findall(pattern1, vocab_section)
        vocab_words.update([w.lower() for w in matches1])
        
        # æ ¼å¼2: **æ•°å­—. word** /pronunciation/
        pattern2 = r'\*\*\d+\.\s*([a-zA-Z]+)\*\*\s*/'
        matches2 = re.findall(pattern2, vocab_section)
        vocab_words.update([w.lower() for w in matches2])
        
        # æ ¼å¼3: **word** /pronunciation/ï¼ˆæ²¡æœ‰æ•°å­—ï¼‰
        pattern3 = r'\*\*([a-zA-Z]+)\*\*\s*/[^/\n]*'
        matches3 = re.findall(pattern3, vocab_section)
        vocab_words.update([w.lower() for w in matches3])
        
        # å¦‚æœä¸Šé¢çš„åŒ¹é…æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„å•è¯ï¼Œå°è¯•åŒ¹é…æ‰€æœ‰ **word** æ ¼å¼ï¼ˆåœ¨è¯æ±‡æ³¨é‡Šéƒ¨åˆ†ï¼‰
        if len(vocab_words) < 10:  # å¦‚æœæå–çš„å•è¯å¤ªå°‘ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            # åŒ¹é…æ‰€æœ‰ **word** æ ¼å¼ï¼Œä½†æ’é™¤ä¸€äº›å¸¸è§çš„éè¯æ±‡å•è¯
            all_bold_pattern = r'\*\*([a-zA-Z]{3,})\*\*'
            all_matches = re.findall(all_bold_pattern, vocab_section)
            # è¿‡æ»¤æ‰ä¸€äº›å¸¸è§çš„éè¯æ±‡å•è¯
            exclude_words = {'ç”¨æ³•', 'è®°å¿†', 'æŠ€å·§', 'ä¾‹å¥', 'è¯æ€§', 'æ ¸å¿ƒ', 'é‡è¦', 'å…¶ä»–', 'try', 'free'}
            vocab_words.update([w.lower() for w in all_matches if w.lower() not in exclude_words])
    
    return vocab_words

def remove_bold_outside_vocab(text, vocab_words):
    """ç§»é™¤æ–‡æœ¬ä¸­ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯çš„åŠ ç²—ï¼Œä¿ç•™è¯æ±‡è¡¨å•è¯çš„åŠ ç²—"""
    def replace_bold(match):
        word = match.group(1)
        word_lower = word.lower()
        # å¦‚æœå•è¯åœ¨è¯æ±‡è¡¨ä¸­ï¼Œä¿ç•™åŠ ç²—
        if word_lower in vocab_words:
            return match.group(0)  # ä¿ç•™åŸæ · **word**
        else:
            return word  # ç§»é™¤åŠ ç²—
    
    # æ›¿æ¢ **word** æ ¼å¼ï¼ˆåªåŒ¹é…å•è¯ï¼Œä¸åŒ…æ‹¬æ ‡ç‚¹ï¼‰
    pattern = r'\*\*([a-zA-Z]+)\*\*'
    text = re.sub(pattern, replace_bold, text)
    
    return text

def remove_all_bold(text):
    """ç§»é™¤æ–‡æœ¬ä¸­æ‰€æœ‰åŠ ç²—"""
    # æ›¿æ¢ **word** ä¸º word
    text = re.sub(r'\*\*(\w+)\*\*', r'\1', text)
    # æ›¿æ¢ **çŸ­è¯­** ä¸º çŸ­è¯­ï¼ˆå¤„ç†ä¸­æ–‡æˆ–çŸ­è¯­ï¼‰
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    return text

def process_file(file_path):
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    print(f"å¤„ç†æ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–è¯æ±‡è¡¨ä¸­çš„å•è¯
    vocab_words = extract_vocabulary_words(content)
    print(f"  æ‰¾åˆ° {len(vocab_words)} ä¸ªè¯æ±‡è¡¨å•è¯")
    
    # æ‰¾åˆ°é˜…è¯»ææ–™éƒ¨åˆ†ï¼ˆä»"## ğŸ“– Reading Passage"å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ª"##"æˆ–"---"ç»“æŸï¼‰
    reading_section_pattern = r'(## ğŸ“– Reading Passage:.*?)(?=---|\n##)'
    reading_match = re.search(reading_section_pattern, content, re.DOTALL)
    
    if reading_match:
        reading_section = reading_match.group(1)
        
        # å¤„ç†é˜…è¯»ææ–™éƒ¨åˆ†ï¼šåªä¿ç•™è¯æ±‡è¡¨å•è¯çš„åŠ ç²—
        processed_reading = remove_bold_outside_vocab(reading_section, vocab_words)
        
        # æ›¿æ¢åŸå†…å®¹
        content = content[:reading_match.start()] + processed_reading + content[reading_match.end():]
        
        # é‡æ–°åŒ¹é…ä»¥æ›´æ–°ä½ç½®
        reading_match = re.search(reading_section_pattern, content, re.DOTALL)
    
    # å¤„ç†å…¶ä»–éƒ¨åˆ†ï¼šå–æ¶ˆæ‰€æœ‰åŠ ç²—
    # æ‰¾åˆ°æ–‡ç« æ¦‚è¦éƒ¨åˆ†ï¼ˆä»"## ğŸ“‹ æ–‡ç« æ¦‚è¦"å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ª"##"æˆ–"---"ç»“æŸï¼‰
    summary_pattern = r'(## ğŸ“‹ æ–‡ç« æ¦‚è¦.*?)(?=---|\n##)'
    summary_match = re.search(summary_pattern, content, re.DOTALL)
    if summary_match:
        summary_section = summary_match.group(1)
        processed_summary = remove_all_bold(summary_section)
        content = content[:summary_match.start()] + processed_summary + content[summary_match.end():]
    
    # å¤„ç†ä¸­æ–‡ç¿»è¯‘éƒ¨åˆ†ï¼ˆä»"## ğŸ“– ä¸­æ–‡ç¿»è¯‘"å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ª"##"æˆ–"---"ç»“æŸï¼‰
    translation_pattern = r'(## ğŸ“– ä¸­æ–‡ç¿»è¯‘.*?)(?=---|\n##)'
    translation_match = re.search(translation_pattern, content, re.DOTALL)
    if translation_match:
        translation_section = translation_match.group(1)
        processed_translation = remove_all_bold(translation_section)
        content = content[:translation_match.start()] + processed_translation + content[translation_match.end():]
    
    # å¤„ç†å…¶ä»–éƒ¨åˆ†ï¼šé™¤äº†è¯æ±‡æ³¨é‡Šéƒ¨åˆ†ï¼Œå…¶ä»–éƒ½å–æ¶ˆåŠ ç²—
    # ä½†æˆ‘ä»¬ä¿ç•™è¯æ±‡æ³¨é‡Šéƒ¨åˆ†çš„åŠ ç²—ï¼ˆå› ä¸ºé‚£æ˜¯è¯æ±‡è¡¨ï¼‰
    
    # ä¿å­˜æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"  å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    reading_materials_dir = Path('reading_materials')
    
    # è·å–æ‰€æœ‰ group*_reading.md æ–‡ä»¶
    files = sorted(reading_materials_dir.glob('group*_reading.md'))
    
    print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†\n")
    
    for file_path in files:
        try:
            process_file(file_path)
        except Exception as e:
            print(f"  å¤„ç†å¤±è´¥: {e}")
    
    print(f"\næ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    main()
