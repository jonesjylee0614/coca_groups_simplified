# Chapter 79: Tech Ethics

Alex sat in his **office** on a gray **morning**, reviewing notes **before** his first interview. The assignment was tech ethics—examining **whether** the technology industry was doing **enough** to **consider** its impact on society. It was a topic that required him to **walk** a careful line between celebration and critique.

His editor Sarah Chen had been clear. "We need to **add** depth to this conversation," she'd said. "Move beyond **public** **political** debates about tech platforms. Show what's happening at ground **level**—the decisions engineers and executives make, and the **result** for ordinary people."

Alex's first interview was with Dr. Maria Santos, a philosophy professor at the **university**. They met at her **office**, where books lined **white** walls from floor to ceiling. Maria had **spend** years studying technology ethics, teaching students to **consider** the implications of innovation **before** writing **such** code.

"The tech industry moves **almost** too fast for ethics to keep up," Maria explained, her voice thoughtful. "Engineers **build** systems, deploy them to millions, and only **later** **consider** consequences. By then, **change** is difficult. The systems are embedded, people depend on them, and there's **little** incentive to **change**. We're not **able** to put the genie back in the bottle easily."

She described teaching a **course** on AI ethics. "My students are brilliant," she said. "They can solve complex technical problems. But many **stay** uncomfortable thinking about **political** or social implications. They want clear answers, but ethics rarely works that way. You have to **walk** through ambiguity."

Maria gave an example: facial recognition technology. "The technical capability exists. But should we **spend** resources building it? Should police use it? Should companies **buy** it? These aren't technical questions—they're ethical ones. And they don't have easy **such** answers."

Alex's next interview was with Marcus Chen, an engineer who'd left a major tech company over ethical concerns. They met at a café near Marcus's new **home**. He'd **spend** **five** years working on ad-targeting algorithms **before** realizing the harm they caused.

"I was good at my job," Marcus said, looking tired. "Too good, maybe. I built systems that could predict user behavior, target ads with precision. The company loved it—revenue grew. But I started seeing the **result**. People manipulated, **political** content weaponized, vulnerable populations targeted."

He described the **moment** he decided to leave. "A colleague showed me data on how our algorithms were being used to target gambling ads at people with addiction **history**. We knew who was vulnerable. We used that knowledge to exploit them. I couldn't **stay** after that."

Marcus now worked for a nonprofit advocating for ethical AI development. "It pays less," he acknowledged. "But I can **walk** through my own **door** at **home** without feeling complicit in harm."

Alex visited a tech company that had made ethics **center** to its mission. The CEO, Jennifer Park, had founded the company specifically to prove that **public** good and profit could coexist. They met in her **office**, which had a view of the city through large windows. She'd just gotten **off** a conference call with investors who didn't always understand her approach.

"Most tech companies treat ethics as an afterthought," Jennifer said bluntly. "Something to **add** later, to **consider** when there's time. We **build** it into everything from the start. Every product decision goes through an ethics review **before** deployment, **including** features that seem minor."

She showed Alex their process. "We ask: Who benefits? Who might be harmed? Are we creating value or just extracting it? Could **such** a system be misused? We don't always have perfect answers, but we **spend** time wrestling with questions."

Her company had turned down profitable opportunities because they failed ethical scrutiny. "We were approached about building surveillance tools for a foreign government," Jennifer explained. "Financially attractive deal. But we couldn't **consider** it seriously given that country's human rights **history**. Some money isn't worth taking."

Alex interviewed employees at a tech company facing **public** backlash over data privacy. He met with **five** current and former workers who described a culture where growth trumped everything **else**. These were **among** the most candid conversations he'd had, with **several** people sharing stories they'd never told publicly.

One engineer, speaking anonymously, described the pressure. "Ship features fast, ask questions **later**. That was the mentality. If users didn't **die** from it, **whether** it was ethical didn't matter. Sounds extreme, but that's how it felt."

Another employee described trying to raise concerns. "I flagged privacy issues **before** a product launch. My manager thanked me, said we'd address it **later**. We never did. The feature shipped. Users had no idea how **little** control they had over their data."

A former content moderator talked about the psychological toll. "I saw the worst of humanity every day. Violence, abuse, exploitation. The company provided **such** minimal support. They knew the work was harmful but didn't **spend** **enough** to protect us."

Alex visited a **research** institution studying technology's societal impact. Dr. Sarah Kim led a team examining how algorithms affected **political** discourse, economic inequality, and social cohesion.

"Technology isn't neutral," Sarah explained, walking Alex through their lab. "Every design choice reflects values, **whether** intentional or not. An algorithm that optimizes for engagement will **probably** amplify outrage—that's what keeps people clicking. Is that the **public** interest? **Almost** certainly not."

Her team's **research** showed troubling patterns. Social media algorithms created filter bubbles. Recommendation systems pushed people toward extreme content. Ad-targeting reinforced discrimination. "The **result** is a more divided, less informed **public**," Sarah said. "And it's **both** predictable and preventable."

She emphasized that engineers had **better** options. "You can **build** systems that prioritize information quality over engagement, that expose people to diverse perspectives, that protect vulnerable users. It requires different priorities, but it's technically possible."

Alex attended a panel discussion on tech ethics featuring representatives from academia, industry, and civil society. The conversation was **political** in the best sense—substantive debate about values and priorities.

One panelist, a **parent** of **both** a young **boy** and girl, spoke about children and technology. "My kids **grow** up surrounded by devices. Algorithms shape what they see, how they think, **such** as what they believe. As a **parent**, I worry. Are these systems designed with their wellbeing in mind? Or just to capture their attention?"

Another panelist, a civil rights lawyer, discussed discrimination in algorithmic systems. "We see it in hiring algorithms, credit scoring, criminal justice. Systems that **consider** **such** factors as zip code or name can perpetuate historical discrimination. The code might be colorblind, but the **result** isn't."

A tech executive defended the industry. "We're trying to do **better**," he insisted. "Ethics is complicated. We **consider** impact, we hire ethicists, we revise policies. But we operate in competitive markets. If we **spend** too much time deliberating, we **stay** behind."

Alex visited a whistleblower who'd exposed unethical practices at their former company. Clara Martinez had lost her job, faced legal threats, and was still dealing with a **court** case the company had brought against her. They met at her small apartment, where she lived quietly now, eating simple **food** she'd prepared.

"I tried working **within** the system," Clara said, taking a **second** to collect her thoughts. "Raised concerns through proper channels. Nothing changed. The company cared more about stock price than doing right. **Such** behavior is common, not exceptional."

She described the **moment** she decided to go **public**. "I saw internal documents showing executives knew about harm and chose profit anyway. People needed to know. The **public** deserves that information."

The backlash had been severe. "I can't get hired in tech now," Clara admitted. "I'm **almost** a pariah. But I'd do it again. Some things matter more than career."

Alex interviewed ethics professors teaching the next generation of engineers. Dr. James Wong taught at a top engineering school, trying to instill ethical thinking in students **before** they entered the industry.

"I **send** them into the world hoping they'll make **better** choices," James said. "But the pressure they'll **face** is enormous. Corporate culture, shareholder expectations, competitive dynamics—all push toward prioritizing profit over ethics."

He described the challenge. "In school, we can **consider** ethical implications carefully. In industry, decisions happen **such** rapidly. Algorithms get deployed in a **moment**. The **course** of events moves faster than deliberation."

Still, James believed education mattered. "**Whether** one person speaks up or stays silent can **change** outcomes. I want my students to have the tools to recognize ethical issues and the courage to **face** them."

Alex visited a nonprofit working to make tech more equitable. The founder, Rebecca Turner, had **spend** years advocating for marginalized communities affected by technology.

"Tech companies claim to serve everyone," Rebecca said. "But their products **almost** always reflect the priorities and assumptions of their creators—**probably** **white**, **probably** male, **probably** affluent. The **result** is technology that works **better** for some than others."

She gave examples: voice recognition that struggled with accents, medical algorithms trained predominantly on **white** patients, hiring tools that discriminated against women. "These aren't accidents," Rebecca argued. "They're predictable outcomes when teams lack diversity and don't **consider** impact on **such** different populations."

Her organization worked with tech companies willing to do **better**. "Some are **open** to **change**," Rebecca said. "They **consider** our feedback, revise products, hire more diversely. But it requires sustained effort, not just surface-level gestures."

At a tech conference, Alex attended talks about ethical AI development. Speakers discussed technical approaches—fairness metrics, bias testing, transparency tools. But the most compelling talk came from a young engineer named David Park.

"I'm **probably** the stereotypical tech worker," David said. "Went to a good **university**, got hired at a major company, write code that affects millions. For a **little** while, I didn't think much about implications. I assumed if something was technically possible and legally permitted, it was okay."

He described a turning point. "My **mother** got targeted by a scam that used AI to clone a relative's voice. She **almost** **send** money **before** realizing it was fake. That's when I understood—**such** technology I help create can harm people I love."

David now led an ethics working group at his company. "We can't prevent every misuse," he acknowledged. "But we can **build** safeguards, **consider** potential harms **before** deployment, **add** friction to prevent casual abuse. It's not perfect, but it's **better** than nothing."

Back at his **office**, Alex worked through his notes. The article wasn't simple—ethics never was. He'd heard from people trying to do **better** and those who'd sacrificed careers for principles. He'd seen companies taking ethics seriously and others treating it as **public** relations.

"This is powerful," Sarah Chen said, reading his draft. "You're showing that ethics isn't abstract—it's about real choices people **face**, real consequences that **result**, real lives affected. And you're showing **both** the progress and the remaining problems."

The article published with the headline: "Tech Ethics: The People Fighting to Make Technology **Better** for Everyone." Response was immediate and predictable. Industry defenders thought he was too critical. Critics thought he was too soft. But many readers recognized themselves in the stories—as **parent**s worried about kids, as users feeling manipulated, as workers **face**ing difficult choices.

One email stood out, from a young engineering student: "I'm about to start my first tech job. Your article reminded me that I'll **face** ethical choices, not just technical ones. I want to be someone who speaks up, who makes **better** decisions. Thank you for showing what that looks like."

Alex saved that message. Technology would continue advancing, new ethical challenges would emerge. But people would **stay** at the **center**—making choices, facing consequences, deciding what kind of world to **build**.

As he walked **home** that evening, Alex thought about the engineers and executives he'd interviewed. They worked in a complicated space where technical capability, business pressure, and ethical responsibility collided. Some navigated it **better** than others, but all **face**d genuine dilemmas.

The **public** conversation about tech ethics had **grow**n more sophisticated. People understood now that technology wasn't neutral, that design choices mattered, that innovation without ethics led to harm. **Whether** that understanding would **change** industry practices remained to be seen.

But journalism had a role—telling stories of those trying to do **better**, exposing those choosing profit over people, helping **such** a **public** understand stakes and make informed demands. Not with easy answers—those didn't exist—but with honest reporting about real challenges.

That work would continue, one article at a time, **add**ing to **public** understanding, pressing for **better** practices, insisting that technology serve human flourishing rather than undermine it.

The assignment had reminded Alex why he became a journalist—to **walk** into complex spaces, ask hard questions, tell stories that matter. Tech ethics was exactly **such** a space, and he was determined to keep covering it.

Tomorrow would bring new stories, new challenges, new choices to **face**. But tonight, Alex felt satisfied with work well done, stories well told, a **public** a **little** **better** informed about technologies shaping their lives.

And in a world where technology changed **almost** everything, that felt like meaningful work worth doing.
